{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def macro_precision(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate macro averaged precision\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: macro precision score\n",
    "    \"\"\"\n",
    "    # find the number of classes by taking\n",
    "    # length of unique values in true list\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize precision to 0\n",
    "    precision = 0\n",
    "    \n",
    "    # loop over all classes \n",
    "    for class_ in range(num_classes):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p==class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p==class_ else 0 for p in y_pred]\n",
    "        \n",
    "        # calculate true positive for current class\n",
    "        tp = true_positive(temp_true,temp_pred)\n",
    "        \n",
    "        # calculate false positive for current class\n",
    "        fp = false_positive(temp_true,temp_pred)\n",
    "        \n",
    "        # caluclate precision for current classes\n",
    "        temp_precision = tp / (tp+fp)\n",
    "        \n",
    "        # keep adding precision for all classes\n",
    "        precision = precision + temp_precision\n",
    "    \n",
    "    # calculate and return average precision over all classes\n",
    "    return precision/num_classes\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def micro_precision(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate micro averaged precision\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: micro precision score\n",
    "    \"\"\"\n",
    "    \n",
    "    # find the number of classes by taking\n",
    "    # length of unique values in true list\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in range(num_classes):\n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        # calculate true positive for current class\n",
    "        # and update overall tp\n",
    "        tp += true_positive(temp_true,temp_pred)\n",
    "        \n",
    "        # calculate false positive for current class\n",
    "        # and update overall tp\n",
    "        fp += false_positive(temp_true,temp_pred)\n",
    "        \n",
    "    # calculate and return overall precision\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def weighted_precision(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate weighted averaged precision\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: weighted precision score\n",
    "    \"\"\"\n",
    "    \n",
    "    # find the number of classes by taking\n",
    "    # length of unique values in true list\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # create class:sample count dictionary\n",
    "    # it looks something like this:\n",
    "    # {0: 20, 1:15, 2:21}\n",
    "    class_counts = Counter(y_true)\n",
    "    \n",
    "    # initialize precision to 0\n",
    "    precision = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in range(num_classes):\n",
    "        temp_true = [1 if p==class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p==class_ else 0 for p in y_pred]\n",
    "        \n",
    "        tp = true_positive(temp_true,temp_pred)\n",
    "        fp = false_positive(temp_true,temp_pred)\n",
    "        \n",
    "        temp_precision = tp/(tp+fp)\n",
    "        \n",
    "        weighted_precision = class_counts[class_]*temp_precision\n",
    "        \n",
    "        precision += weighted_precision\n",
    "        \n",
    "    return precision/len(y_true) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3611111111111111"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n",
    "y_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n",
    "\n",
    "macro_precision(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3611111111111111"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_true,y_pred,average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "micro_precision(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_true,y_pred,average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39814814814814814"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_precision(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39814814814814814"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_true,y_pred,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b084de08457aacf8a82bcc507fec1f14cdbbd842bff9a6dbce8ad81f7549a938"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
